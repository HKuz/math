{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algebra\n",
    "\n",
    "Sources: [Deep Learning](www.deeplearningbook.org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library imports\n",
    "import numpy as np\n",
    "from scipy import linalg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definitions and notation:\n",
    "\n",
    "- **Scalar**: a single number, such as $s \\in \\mathbb{R}$ or $n \\in \\mathbb{N}$.\n",
    "- **Vector**: an array of numbers in order. If each element $x_i \\in \\mathbb{R}$ for vector $\\mathbf{a}$, then vector $\\mathbf{a}$ lies in set $\\mathbb{R}^n$. Vectors in machine learning are typically column vectors (shape $n \\times 1$). You can think of vectors as identifying points in space, with each element giving the coordinate along a diï¬€erent axis.\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{a} = \\sum_{i=1}^n a_i b_i\n",
    "\\end{align}\n",
    "\n",
    "- **Matrix**: 2D array of numbers, each element has two indices. A matrix $\\mathbf{A}$ with $m$ rows and $n$ columns, then $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$. Elements of a matrix are identified as $A_{i, j}$ where the subscripts identify the $i$-th row and $j$-th column for the item.\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{A} = \\begin{bmatrix}\n",
    "A_{1, 1} & A_{1, 2} \\\\\n",
    "A_{2, 1} & A_{2, 2} \n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "- **Tensor**: an array $\\mathsf{A}$ with more than two axes. Elements are identified by $\\mathsf{A}_{i, j, k}$.\n",
    "- **Transpose**: the transpose of a matrix is the mirror image of the matrix across the main diagonal (running down and to right):\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{A} = \\begin{bmatrix}\n",
    "A_{1, 1} & A_{1, 2} \\\\\n",
    "A_{2, 1} & A_{2, 2} \\\\\n",
    "A_{3, 1} & A_{3, 2}\n",
    "\\end{bmatrix} \\Rightarrow\n",
    "\\mathbf{A}^\\mathsf{T} = \\begin{bmatrix}\n",
    "A_{1, 1} & A_{2, 1} & A_{3, 1} \\\\\n",
    "A_{1, 2} & A_{2, 2} & A_{3, 2} \n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "- A **diagonal** matrix consists mostly of zeroes and has entries only along the main diagonal ($\\mathbf{D}$ is diagonal if and only if $\\mathbf{D}_{i, j} = 0$ for all $i \\ne j$). A square diagonal matrix is written as a vector diag($\\mathbf{v}$) which indicates the non-zero entries. They are easy to compute with since diag($\\mathbf{v}$)$\\mathbf{x}$ is just element-wise multiplication between the vectors (or the Hadamard product, defined below). Additionally, assuming all diagonal entries are non-zero, the inverse is diag($\\mathbf{v}\\text{)}^{-1}=$ diag($[1/v_1, \\ldots , 1/v_n]^{\\mathsf{T}}$).\n",
    "- A **symmetric matrix** is one that's equal to its own transpose: $\\mathbf{A} = \\mathbf{A}^{\\mathsf{T}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computations with vectors and matrices:\n",
    "\n",
    "- The **dot product** of two vectors $\\mathbf{a}$ and $\\mathbf{b}$ (which have the same dimensionality) is defined as the sum of the element-wise products:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{a} \\cdot \\mathbf{b} = \\sum_{i=1}^n a_i b_i\n",
    "\\end{align}\n",
    "\n",
    "- **Matrix multiplication** of $A$ and $B$ only works if $A$ has the same number of columns as $B$ has rows. So if $A$ is $m \\times n$ and $B$ is $n \\times p$, the result $C$ is of shape $m \\times p$\n",
    "\n",
    "$$\n",
    "C_{i, j} = \\sum_k \\mathbf{A}_{i, k} \\mathbf{B}_{k, j}\n",
    "$$\n",
    "\n",
    "- There is an element-wise product defined for two matrices, which is the **Hadamard product** and is denoted ($\\mathbf{A} \\odot \\mathbf{B}$)\n",
    "- A system of equations can be written as $\\mathbf{Ax} = \\mathbf{b}$, where $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ is a known matrix, $\\mathbf{b} \\in \\mathbb{R}^{m}$ is a known vector, and $\\mathbf{x} \\in \\mathbb{R}^{n}$ is a vector of unknown variables to solve for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector a: [1 2 3 4]\n",
      "Vector b: [1 0 2 1]\n",
      "Dot product of a and b: 11\n"
     ]
    }
   ],
   "source": [
    "# Vectors and dot products\n",
    "a = np.array([1, 2, 3, 4]).reshape(4,)\n",
    "print('Vector a:', a)\n",
    "\n",
    "b = np.array([1, 0, 2, 1]).reshape(4,)\n",
    "print('Vector b:', b)\n",
    "\n",
    "print('Dot product of a and b:', np.dot(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A:\n",
      "[[ 5  2]\n",
      " [10  1]\n",
      " [ 0  7]]\n",
      "Matrix B:\n",
      "[[1 3]\n",
      " [0 1]]\n",
      "AB =\n",
      "[[ 5 17]\n",
      " [10 31]\n",
      " [ 0  7]]\n"
     ]
    }
   ],
   "source": [
    "# Matrix multiplication\n",
    "A = np.array([5, 2, 10, 1, 0, 7]).reshape(3, 2)\n",
    "print('Matrix A:')\n",
    "print(A)\n",
    "\n",
    "B = np.array([1, 3, 0, 1]).reshape(2, 2)\n",
    "print('Matrix B:')\n",
    "print(B)\n",
    "\n",
    "print('AB =')\n",
    "print(np.matmul(A, B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Multiplication Properties\n",
    "\n",
    "Matrix multiplication is both distributive $\\mathbf{A}(\\mathbf{B} + \\mathbf{C}) = \\mathbf{A}\\mathbf{B} + \\mathbf{A}\\mathbf{C}$ as well as associative $\\mathbf{A}(\\mathbf{B} \\mathbf{C}) = (\\mathbf{A}\\mathbf{B}) \\mathbf{C}$.\n",
    "\n",
    "However, matrix multiplication is NOT commutative $\\mathbf{A} \\mathbf{B} \\ne \\mathbf{B} \\mathbf{A}$. That said, the dot product between two vectors is commutative: $\\mathbf{x}^{\\mathsf{T}} \\mathbf{y} = \\mathbf{y}^{\\mathsf{T}} \\mathbf{x}$.\n",
    "\n",
    "The transpose of a matrix product can be written as $\\mathbf{AB}^{\\mathsf{T}} = \\mathbf{B}^{\\mathsf{T}} \\mathbf{A}^{\\mathsf{T}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A:\n",
      "[[ 5  2]\n",
      " [10  1]\n",
      " [ 0  7]]\n",
      "Transpose of A:\n",
      "[[ 5 10  0]\n",
      " [ 2  1  7]]\n"
     ]
    }
   ],
   "source": [
    "# Transposes\n",
    "print('Matrix A:')\n",
    "print(A)\n",
    "print('Transpose of A:')\n",
    "print(A.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector a:\n",
      "Vector b:\n",
      "Tranpose of a dot b:\n",
      "11\n",
      "Tranpose of b dot a:\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "print('Vector a:')\n",
    "print('Vector b:')\n",
    "print('Tranpose of a dot b:')\n",
    "print(np.dot(a.T, b))\n",
    "print('Tranpose of b dot a:')\n",
    "print(np.dot(b.T, a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A(B + C):\n",
      "[[25 39]\n",
      " [50 72]\n",
      " [ 0 14]]\n",
      "AB + AC:\n",
      "[[25 39]\n",
      " [50 72]\n",
      " [ 0 14]]\n"
     ]
    }
   ],
   "source": [
    "# Distributive property\n",
    "C = np.array([4, 4, 0, 1]).reshape(2, 2)\n",
    "print('A(B + C):')\n",
    "print(np.matmul(A, B+C))\n",
    "\n",
    "print('AB + AC:')\n",
    "print(np.matmul(A, B) + np.matmul(A, C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A(BC):\n",
      "[[20 37]\n",
      " [40 71]\n",
      " [ 0  7]]\n",
      "(AB)C:\n",
      "[[20 37]\n",
      " [40 71]\n",
      " [ 0  7]]\n"
     ]
    }
   ],
   "source": [
    "# Associative property\n",
    "print('A(BC):')\n",
    "print(np.matmul(A, np.matmul(B, C)))\n",
    "\n",
    "print('(AB)C:')\n",
    "print(np.matmul(np.matmul(A, B), C))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identity and Inverse Matrices\n",
    "\n",
    "The **identity matrix** is a matrix that does not change any vector when you multiply the vector by that matrix. The identity matrix that preserves $n$-dimensional vectors is denoted $\\mathbf{I}_n$:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{I}_n \\in \\mathbb{R}^{n \\times n} \\text{and } \\forall \\mathbf{x} \\in \\mathbb{R}^n, \\, \\mathbf{I}_n \\mathbf{x} = \\mathbf{x}\n",
    "\\end{align}\n",
    "\n",
    "The structure of an identity matrix has $1$'s along the main diagonal and zeroes for all other entries. For example:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{I}_3 = \\begin{bmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "A **matrix inverse** of $\\mathbf{A}$ is written as $\\mathbf{A}^{-1}$ and is defined so $\\mathbf{A}^{-1} \\mathbf{A} = \\mathbf{I}_n$. It's also possible to define an inverse that's multiplied on the right, such that $\\mathbf{A} \\mathbf{A}^{-1} = \\mathbf{I}_n$. For square matrices ($m = n$), the left and right inverses are the same.\n",
    "\n",
    "This is useful in theory to solve a system of linear equations $\\mathbf{Ax} = \\mathbf{b}$, where the solution is $\\mathbf{x} = \\mathbf{A}^{-1} \\mathbf{b}$. This assumes that the inverse exists, for that to happen, the equation $\\mathbf{Ax} = \\mathbf{b}$ has exactly one solution (versus no solutions or an infinite number of solutions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identity matrix (4x4):\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]]\n",
      "Vector a:\n",
      "[1 2 3 4]\n",
      "I_4 * a:\n",
      "[1. 2. 3. 4.]\n"
     ]
    }
   ],
   "source": [
    "I4 = np.eye(4)\n",
    "print('Identity matrix (4x4):')\n",
    "print(I4)\n",
    "\n",
    "print('Vector a:')\n",
    "print(a)\n",
    "\n",
    "print('I_4 * a:')\n",
    "print(np.matmul(I4, a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Combinations and Span\n",
    "\n",
    "- A **linear combination** of a set of vectors $\\{\\mathbf{v}^{(1)}, \\ldots , \\mathbf{v}^{(n)} \\}$ is given be multiplying each vector $\\mathbf{v}^{(i)}$ by a scalar and adding the results:\n",
    "\n",
    "\\begin{align}\n",
    "\\displaystyle \\sum_i = c_i \\mathbf{v}^{(i)}\n",
    "\\end{align}\n",
    "\n",
    "- The **span** of a set of vectors is the set of all points obtainable by linear combination of the original vectors.\n",
    "\n",
    "Finding whether $\\mathbf{Ax} = \\mathbf{b}$ has a solution boils down to the following:\n",
    "\n",
    "- $\\mathbf{b}$ is in the span of the columns of $\\mathbf{A}$ (also known as the **column space**, or **range** of $\\mathbf{A}$)\n",
    "- If $\\mathbf{b} \\in \\mathbb{R}^m$, then the column space of $\\mathbf{A}$ is all of $\\mathbb{R}^m$. (If not, there's a potential value of $\\mathbf{b}$ with no solution)\n",
    "    - This implies that $\\mathbf{A}$ have at least $m$ columns - $n \\ge m$ (the matrix is at least as wide as it is tall) - otherwise the dimensionality of the column space would be less than $m$. For example, if $\\mathbf{A}$ has shape $3 \\times 2$ (so $\\mathbf{b} \\in \\mathbb{R}^3$), it would be a system of $3$ equations with only $2$ unknown variables. At best, this could trace out a plane in $\\mathbb{R}^3$, so the system would only have a solution if $\\mathbf{b}$ fell on that plane. Note: the condition $n \\ge m$ is only necessary for every point to have a solution, but doesn't guarantee that columns are independent (not redundant)\n",
    "    - The columns must be **linearly independent** (a set of vectors is linearly independent if no vector is a linear combination of other vectors in the set - if you added a vector to the set that were a linear combination of others, it would not add points to the set's span)\n",
    "    - Therefore, for the column space to encompass all $\\mathbb{R}^m$, it must have a set of $m$ linearly independent columns\n",
    "- To guarantee that the matrix $\\mathbf{A}$ has an inverse, there must be *at most* one solution for each value of $\\mathbf{b}$. To do so, $\\mathbf{A}$ must have exactly $m$ columns (otherwise, there can be more than one way of parameterizing the solution)\n",
    "\n",
    "To summarize, in order to find the solution of the system of linear equations using the inverse, $\\mathbf{A}$ must be a **square matrix** ($m = n$) and all columns are linearly independent. If $\\mathbf{A}$ isn't square, or is square but has linearly dependent columns (known as a **singular**), it's still possible to solve, but just not using the matrix inversion technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n",
      "[[1. 2.]\n",
      " [7. 1.]]\n",
      "b:\n",
      "[ 1. 20.]\n",
      "\n",
      "Inverse solution:\n",
      "[ 3. -1.]\n",
      "\n",
      "Scipy sover solution:\n",
      "[ 3. -1.]\n",
      "\n",
      "Check\n",
      "1.0\n",
      "19.999999999999996\n"
     ]
    }
   ],
   "source": [
    "# Using scipy to solve a system of linear equations\n",
    "solve_A = np.array([1., 2., 7., 1.]).reshape(2, 2)\n",
    "solve_b = np.array([1., 20.])\n",
    "print('A:')\n",
    "print(solve_A)\n",
    "print('b:')\n",
    "print(solve_b)\n",
    "print()\n",
    "\n",
    "# Get inverse of A\n",
    "solve_A_inv = linalg.inv(solve_A)\n",
    "\n",
    "solution = np.matmul(solve_A_inv, solve_b)\n",
    "print('Inverse solution:')\n",
    "print(solution)\n",
    "print()\n",
    "\n",
    "# Use scipy's solver\n",
    "alt_sol = linalg.solve(solve_A, solve_b)\n",
    "print('Scipy sover solution:')\n",
    "print(alt_sol)\n",
    "print()\n",
    "\n",
    "#Check\n",
    "print('Check')\n",
    "print(solve_A[0, 0] * solution[0] + solve_A[0, 1] * solution[1])\n",
    "print(solve_A[1, 0] * solution[0] + solve_A[1, 1] * solution[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Norms\n",
    "\n",
    "- A function for the size of a vector is called a **norm**, or more formally, norms are functions that map vectors to non-negative values. The $L^p$ norm for $p \\in \\mathbb{R}$, $p \\ge 1$ is:\n",
    "\n",
    "\\begin{align}\n",
    "\\Vert \\mathbf{x} \\Vert_p = \\left( \\displaystyle \\sum_i \\vert x_i \\vert^p \\right)^{\\frac{1}{p}}\n",
    "\\end{align}\n",
    "\n",
    "- The $L^2$ norm ($p = 2$) is the **Euclidean norm**, and is so common it can be seen written as $\\Vert \\mathbf{x} \\Vert$. Another common norm is using the square of the $L^2$ norm, or simply $\\mathbf{x}^{\\mathsf{T}} \\mathbf{x}$\n",
    "- A **unit vector** is one with unit norm: $\\Vert \\mathbf{x} \\Vert_2 = 1$\n",
    "- The $L^1$ norm (which is the sum of the absolute values of vector entries) is important in machine learning when the difference between zero and non-zero elements is important.\n",
    "\n",
    "\\begin{align}\n",
    "\\Vert \\mathbf{x} \\Vert_1 = \\displaystyle \\sum_i \\vert x_i \\vert\n",
    "\\end{align}\n",
    "\n",
    "- Occasionally the $L^{\\infty}$ is used, which is defined as:\n",
    "\n",
    "\\begin{align}\n",
    "\\Vert \\mathbf{x} \\Vert_{\\infty} = \\text{max}_i \\vert x_i \\vert\n",
    "\\end{align}\n",
    "\n",
    "- In deep learning, the **Frobenius norm** is used to calculate the size of a matrix\n",
    "\n",
    "\\begin{align}\n",
    "\\Vert \\mathbf{A} \\Vert_F = \\sqrt{\\displaystyle \\sum_{i, j} A_{i, j}^2}\n",
    "\\end{align}\n",
    "\n",
    "- The dot product in terms of norms, where $\\theta$ is the angle between the vectors, is:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{x}^{\\mathsf{T}} \\mathbf{y} = \\Vert \\mathbf{x} \\Vert_2 \\Vert \\mathbf{y} \\Vert_2 \\cos \\theta\n",
    "\\end{align}\n",
    "\n",
    "- A vector $\\mathbf{x}$ and a vector $\\mathbf{y}$ are **orthogonal** to each other if $\\mathbf{x}^{\\mathsf{T}} \\mathbf{y} = 0$ (assuming both vectors have non-zero norms), which means they are at a $90^{\\circ}$ angle to each other. In $\\mathbb{R}^n$, at most $n$ vectors with non-zero norms can be mutually orthogonal.\n",
    "- **Orthonormal** vectors are both orthogonal and have unit norms.\n",
    "- An **orthogonal matrix** is a square matrix where the rows are mutually orthonormal and the columns are mutually orthonormal:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{A}^{\\mathsf{T}} \\mathbf{A} = \\mathbf{A} \\mathbf{A}^{\\mathsf{T}} = \\mathbf{I} \\\\\n",
    "\\text{and } \\mathbf{A}^{-1} = \\mathbf{A}^{\\mathsf{T}}\n",
    "\\end{align}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
