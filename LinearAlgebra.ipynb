{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algebra\n",
    "\n",
    "Sources: [Deep Learning](www.deeplearningbook.org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library imports\n",
    "import numpy as np\n",
    "from scipy import linalg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definitions and notation:\n",
    "\n",
    "- **Scalar**: a single number, such as $s \\in \\mathbb{R}$ or $n \\in \\mathbb{N}$.\n",
    "- **Vector**: an array of numbers in order. If each element $x_i \\in \\mathbb{R}$ for vector $\\mathbf{a}$, then vector $\\mathbf{a}$ lies in set $\\mathbb{R}^n$. Vectors in machine learning are typically column vectors (shape $n \\times 1$). You can think of vectors as identifying points in space, with each element giving the coordinate along a diï¬€erent axis.\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{a} = \\sum_{i=1}^n a_i b_i\n",
    "\\end{align}\n",
    "\n",
    "- **Matrix**: 2D array of numbers, each element has two indices. A matrix $\\mathbf{A}$ with $m$ rows and $n$ columns, then $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$. Elements of a matrix are identified as $A_{i, j}$ where the subscripts identify the $i$-th row and $j$-th column for the item.\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{A} = \\begin{bmatrix}\n",
    "A_{1, 1} & A_{1, 2} \\\\\n",
    "A_{2, 1} & A_{2, 2} \n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "- **Tensor**: an array $\\mathsf{A}$ with more than two axes. Elements are identified by $\\mathsf{A}_{i, j, k}$.\n",
    "- **Transpose**: the transpose of a matrix is the mirror image of the matrix across the main diagonal (running down and to right):\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{A} = \\begin{bmatrix}\n",
    "A_{1, 1} & A_{1, 2} \\\\\n",
    "A_{2, 1} & A_{2, 2} \\\\\n",
    "A_{3, 1} & A_{3, 2}\n",
    "\\end{bmatrix} \\Rightarrow\n",
    "\\mathbf{A}^{\\operatorname{T}} = \\begin{bmatrix}\n",
    "A_{1, 1} & A_{2, 1} & A_{3, 1} \\\\\n",
    "A_{1, 2} & A_{2, 2} & A_{3, 2} \n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "- A **diagonal** matrix consists mostly of zeroes and has entries only along the main diagonal ($\\mathbf{D}$ is diagonal if and only if $\\mathbf{D}_{i, j} = 0$ for all $i \\ne j$). A square diagonal matrix is written as a vector diag($\\mathbf{v}$) which indicates the non-zero entries. They are easy to compute with since diag($\\mathbf{v}$)$\\mathbf{x}$ is just element-wise multiplication between the vectors (or the Hadamard product, defined below). Additionally, assuming all diagonal entries are non-zero, the inverse is diag($\\mathbf{v}\\text{)}^{-1}=$ diag($[1/v_1, \\ldots , 1/v_n]^{\\operatorname{T}}$).\n",
    "- A **symmetric matrix** is one that's equal to its own transpose: $\\mathbf{A} = \\mathbf{A}^{\\operatorname{T}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computations with vectors and matrices:\n",
    "\n",
    "- The **dot product** of two vectors $\\mathbf{a}$ and $\\mathbf{b}$ (which have the same dimensionality) is defined as the sum of the element-wise products:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{a} \\cdot \\mathbf{b} = \\sum_{i=1}^n a_i b_i\n",
    "\\end{align}\n",
    "\n",
    "- **Matrix multiplication** of $A$ and $B$ only works if $A$ has the same number of columns as $B$ has rows. So if $A$ is $m \\times n$ and $B$ is $n \\times p$, the result $C$ is of shape $m \\times p$\n",
    "\n",
    "$$\n",
    "C_{i, j} = \\sum_k \\mathbf{A}_{i, k} \\mathbf{B}_{k, j}\n",
    "$$\n",
    "\n",
    "- There is an element-wise product defined for two matrices, which is the **Hadamard product** and is denoted ($\\mathbf{A} \\odot \\mathbf{B}$)\n",
    "- A system of equations can be written as $\\mathbf{Ax} = \\mathbf{b}$, where $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ is a known matrix, $\\mathbf{b} \\in \\mathbb{R}^{m}$ is a known vector, and $\\mathbf{x} \\in \\mathbb{R}^{n}$ is a vector of unknown variables to solve for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector a: [1 2 3 4]\n",
      "Vector b: [1 0 2 1]\n",
      "Dot product of a and b: 11\n"
     ]
    }
   ],
   "source": [
    "# Vectors and dot products\n",
    "a = np.array([1, 2, 3, 4]).reshape(4,)\n",
    "print('Vector a:', a)\n",
    "\n",
    "b = np.array([1, 0, 2, 1]).reshape(4,)\n",
    "print('Vector b:', b)\n",
    "\n",
    "print('Dot product of a and b:', np.dot(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A:\n",
      "[[ 5  2]\n",
      " [10  1]\n",
      " [ 0  7]]\n",
      "Matrix B:\n",
      "[[1 3]\n",
      " [0 1]]\n",
      "AB =\n",
      "[[ 5 17]\n",
      " [10 31]\n",
      " [ 0  7]]\n"
     ]
    }
   ],
   "source": [
    "# Matrix multiplication\n",
    "A = np.array([5, 2, 10, 1, 0, 7]).reshape(3, 2)\n",
    "print('Matrix A:')\n",
    "print(A)\n",
    "\n",
    "B = np.array([1, 3, 0, 1]).reshape(2, 2)\n",
    "print('Matrix B:')\n",
    "print(B)\n",
    "\n",
    "print('AB =')\n",
    "print(np.matmul(A, B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Multiplication Properties\n",
    "\n",
    "Matrix multiplication is both distributive $\\mathbf{A}(\\mathbf{B} + \\mathbf{C}) = \\mathbf{A}\\mathbf{B} + \\mathbf{A}\\mathbf{C}$ as well as associative $\\mathbf{A}(\\mathbf{B} \\mathbf{C}) = (\\mathbf{A}\\mathbf{B}) \\mathbf{C}$.\n",
    "\n",
    "However, matrix multiplication is NOT commutative $\\mathbf{A} \\mathbf{B} \\ne \\mathbf{B} \\mathbf{A}$. That said, the dot product between two vectors is commutative: $\\mathbf{x}^{\\operatorname{T}} \\mathbf{y} = \\mathbf{y}^{\\operatorname{T}} \\mathbf{x}$.\n",
    "\n",
    "The transpose of a matrix product can be written as $\\mathbf{AB}^{\\operatorname{T}} = \\mathbf{B}^{\\operatorname{T}} \\mathbf{A}^{\\operatorname{T}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A:\n",
      "[[ 5  2]\n",
      " [10  1]\n",
      " [ 0  7]]\n",
      "Transpose of A:\n",
      "[[ 5 10  0]\n",
      " [ 2  1  7]]\n"
     ]
    }
   ],
   "source": [
    "# Transpose examples\n",
    "print('Matrix A:')\n",
    "print(A)\n",
    "print('Transpose of A:')\n",
    "print(A.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector a:\n",
      "Vector b:\n",
      "Tranpose of a dot b:\n",
      "11\n",
      "Tranpose of b dot a:\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "# Dot product examples\n",
    "print('Vector a:')\n",
    "print('Vector b:')\n",
    "print('Tranpose of a dot b:')\n",
    "print(np.dot(a.T, b))\n",
    "print('Tranpose of b dot a:')\n",
    "print(np.dot(b.T, a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A(B + C):\n",
      "[[25 39]\n",
      " [50 72]\n",
      " [ 0 14]]\n",
      "AB + AC:\n",
      "[[25 39]\n",
      " [50 72]\n",
      " [ 0 14]]\n"
     ]
    }
   ],
   "source": [
    "# Distributive property example\n",
    "C = np.array([4, 4, 0, 1]).reshape(2, 2)\n",
    "print('A(B + C):')\n",
    "print(np.matmul(A, B+C))\n",
    "\n",
    "print('AB + AC:')\n",
    "print(np.matmul(A, B) + np.matmul(A, C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A(BC):\n",
      "[[20 37]\n",
      " [40 71]\n",
      " [ 0  7]]\n",
      "(AB)C:\n",
      "[[20 37]\n",
      " [40 71]\n",
      " [ 0  7]]\n"
     ]
    }
   ],
   "source": [
    "# Associative property example\n",
    "print('A(BC):')\n",
    "print(np.matmul(A, np.matmul(B, C)))\n",
    "\n",
    "print('(AB)C:')\n",
    "print(np.matmul(np.matmul(A, B), C))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identity and Inverse Matrices\n",
    "\n",
    "The **identity matrix** is a matrix that does not change any vector when you multiply the vector by that matrix. The identity matrix that preserves $n$-dimensional vectors is denoted $\\mathbf{I}_n$:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{I}_n \\in \\mathbb{R}^{n \\times n} \\text{and } \\forall \\mathbf{x} \\in \\mathbb{R}^n, \\, \\mathbf{I}_n \\mathbf{x} = \\mathbf{x}\n",
    "\\end{align}\n",
    "\n",
    "The structure of an identity matrix has $1$'s along the main diagonal and zeroes for all other entries. For example:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{I}_3 = \\begin{bmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "A **matrix inverse** of $\\mathbf{A}$ is written as $\\mathbf{A}^{-1}$ and is defined so $\\mathbf{A}^{-1} \\mathbf{A} = \\mathbf{I}_n$. It's also possible to define an inverse that's multiplied on the right, such that $\\mathbf{A} \\mathbf{A}^{-1} = \\mathbf{I}_n$. For square matrices ($m = n$), the left and right inverses are the same.\n",
    "\n",
    "This is useful in theory to solve a system of linear equations $\\mathbf{Ax} = \\mathbf{b}$, where the solution is $\\mathbf{x} = \\mathbf{A}^{-1} \\mathbf{b}$. This assumes that the inverse exists, for that to happen, the equation $\\mathbf{Ax} = \\mathbf{b}$ has exactly one solution (versus no solutions or an infinite number of solutions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identity matrix (4x4):\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]]\n",
      "\n",
      "Vector a:\n",
      "[1 2 3 4]\n",
      "\n",
      "I_4 * a:\n",
      "[1. 2. 3. 4.]\n"
     ]
    }
   ],
   "source": [
    "# Identity and inverse examples\n",
    "I4 = np.eye(4)\n",
    "print('Identity matrix (4x4):')\n",
    "print(I4)\n",
    "print()\n",
    "\n",
    "print('Vector a:')\n",
    "print(a)\n",
    "print()\n",
    "\n",
    "print('I_4 * a:')\n",
    "print(np.matmul(I4, a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Combinations and Span\n",
    "\n",
    "- A **linear combination** of a set of vectors $\\{\\mathbf{v}^{(1)}, \\ldots , \\mathbf{v}^{(n)} \\}$ is given be multiplying each vector $\\mathbf{v}^{(i)}$ by a scalar and adding the results:\n",
    "\n",
    "\\begin{align}\n",
    "\\displaystyle \\sum_i = c_i \\mathbf{v}^{(i)}\n",
    "\\end{align}\n",
    "\n",
    "- The **span** of a set of vectors is the set of all points obtainable by linear combination of the original vectors.\n",
    "\n",
    "Finding whether $\\mathbf{Ax} = \\mathbf{b}$ has a solution boils down to the following:\n",
    "\n",
    "- $\\mathbf{b}$ is in the span of the columns of $\\mathbf{A}$ (also known as the **column space**, or **range** of $\\mathbf{A}$)\n",
    "- If $\\mathbf{b} \\in \\mathbb{R}^m$, then the column space of $\\mathbf{A}$ is all of $\\mathbb{R}^m$. (If not, there's a potential value of $\\mathbf{b}$ with no solution)\n",
    "    - This implies that $\\mathbf{A}$ have at least $m$ columns - $n \\ge m$ (the matrix is at least as wide as it is tall) - otherwise the dimensionality of the column space would be less than $m$. For example, if $\\mathbf{A}$ has shape $3 \\times 2$ (so $\\mathbf{b} \\in \\mathbb{R}^3$), it would be a system of $3$ equations with only $2$ unknown variables. At best, this could trace out a plane in $\\mathbb{R}^3$, so the system would only have a solution if $\\mathbf{b}$ fell on that plane. Note: the condition $n \\ge m$ is only necessary for every point to have a solution, but doesn't guarantee that columns are independent (not redundant)\n",
    "    - The columns must be **linearly independent** (a set of vectors is linearly independent if no vector is a linear combination of other vectors in the set - if you added a vector to the set that were a linear combination of others, it would not add points to the set's span)\n",
    "    - Therefore, for the column space to encompass all $\\mathbb{R}^m$, it must have a set of $m$ linearly independent columns\n",
    "- To guarantee that the matrix $\\mathbf{A}$ has an inverse, there must be *at most* one solution for each value of $\\mathbf{b}$. To do so, $\\mathbf{A}$ must have exactly $m$ columns (otherwise, there can be more than one way of parameterizing the solution)\n",
    "\n",
    "To summarize, in order to find the solution of the system of linear equations using the inverse, $\\mathbf{A}$ must be a **square matrix** ($m = n$) and all columns are linearly independent. If $\\mathbf{A}$ isn't square, or is square but has linearly dependent columns (known as a **singular**), it's still possible to solve, but just not using the matrix inversion technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n",
      "[[1. 2.]\n",
      " [7. 1.]]\n",
      "b:\n",
      "[ 1. 20.]\n",
      "\n",
      "Inverse solution:\n",
      "[ 3. -1.]\n",
      "\n",
      "Scipy sover solution:\n",
      "[ 3. -1.]\n",
      "\n",
      "Check\n",
      "1.0\n",
      "19.999999999999996\n"
     ]
    }
   ],
   "source": [
    "# Using scipy to solve a system of linear equations\n",
    "solve_A = np.array([1., 2., 7., 1.]).reshape(2, 2)\n",
    "solve_b = np.array([1., 20.])\n",
    "print('A:')\n",
    "print(solve_A)\n",
    "print('b:')\n",
    "print(solve_b)\n",
    "print()\n",
    "\n",
    "# Get inverse of A\n",
    "solve_A_inv = linalg.inv(solve_A)\n",
    "\n",
    "solution = np.matmul(solve_A_inv, solve_b)\n",
    "print('Inverse solution:')\n",
    "print(solution)\n",
    "print()\n",
    "\n",
    "# Use scipy's solver\n",
    "alt_sol = linalg.solve(solve_A, solve_b)\n",
    "print('Scipy sover solution:')\n",
    "print(alt_sol)\n",
    "print()\n",
    "\n",
    "#Check\n",
    "print('Check')\n",
    "print(solve_A[0, 0] * solution[0] + solve_A[0, 1] * solution[1])\n",
    "print(solve_A[1, 0] * solution[0] + solve_A[1, 1] * solution[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Norms\n",
    "\n",
    "- A function for the size of a vector is called a **norm**, or more formally, norms are functions that map vectors to non-negative values. The $L^p$ norm for $p \\in \\mathbb{R}$, $p \\ge 1$ is:\n",
    "\n",
    "\\begin{align}\n",
    "\\Vert \\mathbf{x} \\Vert_p = \\left( \\displaystyle \\sum_i \\vert x_i \\vert^p \\right)^{\\frac{1}{p}}\n",
    "\\end{align}\n",
    "\n",
    "- The $L^2$ norm ($p = 2$) is the **Euclidean norm**, and is so common it can be seen written as $\\Vert \\mathbf{x} \\Vert$. Another common norm is using the square of the $L^2$ norm, or simply $\\mathbf{x}^{\\mathsf{T}} \\mathbf{x}$\n",
    "- A **unit vector** is one with unit norm: $\\Vert \\mathbf{x} \\Vert_2 = 1$\n",
    "- The $L^1$ norm (which is the sum of the absolute values of vector entries) is important in machine learning when the difference between zero and non-zero elements is important.\n",
    "\n",
    "\\begin{align}\n",
    "\\Vert \\mathbf{x} \\Vert_1 = \\displaystyle \\sum_i \\vert x_i \\vert\n",
    "\\end{align}\n",
    "\n",
    "- Occasionally the $L^{\\infty}$ is used, which is defined as:\n",
    "\n",
    "\\begin{align}\n",
    "\\Vert \\mathbf{x} \\Vert_{\\infty} = \\text{max}_i \\vert x_i \\vert\n",
    "\\end{align}\n",
    "\n",
    "- In deep learning, the **Frobenius norm** is used to calculate the size of a matrix\n",
    "\n",
    "\\begin{align}\n",
    "\\Vert \\mathbf{A} \\Vert_F = \\sqrt{\\displaystyle \\sum_{i, j} A_{i, j}^2}\n",
    "\\end{align}\n",
    "\n",
    "- The dot product in terms of norms, where $\\theta$ is the angle between the vectors, is:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{x}^{\\operatorname{T}} \\mathbf{y} = \\Vert \\mathbf{x} \\Vert_2 \\Vert \\mathbf{y} \\Vert_2 \\cos \\theta\n",
    "\\end{align}\n",
    "\n",
    "- A vector $\\mathbf{x}$ and a vector $\\mathbf{y}$ are **orthogonal** to each other if $\\mathbf{x}^{\\operatorname{T}} \\mathbf{y} = 0$ (assuming both vectors have non-zero norms), which means they are at a $90^{\\circ}$ angle to each other. In $\\mathbb{R}^n$, at most $n$ vectors with non-zero norms can be mutually orthogonal.\n",
    "- **Orthonormal** vectors are both orthogonal and have unit norms.\n",
    "- An **orthogonal matrix** is a square matrix where the rows are mutually orthonormal and the columns are mutually orthonormal:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{A}^{\\operatorname{T}} \\mathbf{A} = \\mathbf{A} \\mathbf{A}^{\\operatorname{T}} = \\mathbf{I} \\\\\n",
    "\\text{and } \\mathbf{A}^{-1} = \\mathbf{A}^{\\operatorname{T}}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector a:\n",
      "[1 2 3 4]\n",
      "\n",
      "L2 norm for a:\n",
      "5.477225575051661\n",
      "\n",
      "L-inf norm (max of abs values) for a:\n",
      "4.0\n",
      "\n",
      "Matrix A:\n",
      "[[ 5  2]\n",
      " [10  1]\n",
      " [ 0  7]]\n",
      "\n",
      "Frobenius norm for A:\n",
      "13.379088160259652\n"
     ]
    }
   ],
   "source": [
    "# Norm examples\n",
    "a_norm_l2 = linalg.norm(a)  # default is L2 for vector; Frobenius for matrix\n",
    "a_norm_max = linalg.norm(a, ord=np.inf)\n",
    "A_norm_frob = linalg.norm(A)\n",
    "\n",
    "print('Vector a:')\n",
    "print(a)\n",
    "print()\n",
    "\n",
    "print('L2 norm for a:')\n",
    "print(a_norm_l2)\n",
    "print()\n",
    "\n",
    "print('L-inf norm (max of abs values) for a:')\n",
    "print(a_norm_max)\n",
    "print()\n",
    "\n",
    "print('Matrix A:')\n",
    "print(A)\n",
    "print()\n",
    "\n",
    "print('Frobenius norm for A:')\n",
    "print(A_norm_frob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigenvalues, Eigenvectors, and Eigendecomposition\n",
    "\n",
    "- An **eigenvector** of a square matrix $\\mathbf{A}$ is a nonzero vector $\\mathbf{v}$ such that multiplication by $\\mathbf{A}$ alters only the scale of $\\mathbf{v}$:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{Av} = \\lambda \\mathbf{v}\n",
    "\\end{align}\n",
    "\n",
    "- The scalar $\\lambda$ is known as the **eigenvalue** that corresponds to the above eigenvector\n",
    "- Generally, you focus on unit eigenvectors. If $\\mathbf{v}$ is an eigenvector of $\\mathbf{A}$, then any rescaled vector $s\\mathbf{v}$ for $s \\in \\mathbb{R}, \\, s \\ne 0$ is also an eigenvector with the same $\\lambda$\n",
    "\n",
    "Another way to look at eigenvectors and eigenvalues is through the lens that matrix multiplication applies a linear transformation to that original matrix $\\mathbf{A}$. That transformation may rotate, flip, or stretch space in some way, but the eigenvectors of $\\mathbf{A}$ are special, in that they'll remain on their original span, just scaled by $\\lambda$ after the transformation. To find the eigenvectors, first re-write the right side of the above equation from scalar multiplication to matrix multiplication, and rearrange:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{Av} = (\\lambda \\mathbf{I}) \\mathbf{v} \\\\\n",
    "\\mathbf{Av} - (\\lambda \\mathbf{I}) \\mathbf{v} = \\mathbf{0} \\\\\n",
    "(\\mathbf{A} - \\lambda \\mathbf{I}) \\mathbf{v} = \\mathbf{0}\n",
    "\\end{align}\n",
    "\n",
    "The resulting equation shows $\\mathbf{A}$ less the $\\lambda$ value on the main diagonal multiplied by vector $\\mathbf{v}$ gives the zero vector. When a nonzero matrix times a nonzero vector results in the zero vector, that means the determinant of the matrix must be zero. To find the eigenvectors and eigenvalues for $\\mathbf{A}$, solve for $\\text{det(}\\mathbf{A} - \\lambda \\mathbf{I}\\text{)} = 0$\n",
    "\n",
    "- **Eigendecomposition** decomposes a matrix into a set of eigenvectors and eigenvalues. If you concatenate all eigenvectors $\\mathbf{v}^{(i)}$ into a matrix $\\mathbf{V} = [\\mathbf{v}^{(1)}, \\ldots , \\mathbf{v}^{(n)}]$ and all eigenvalues into a vector $\\mathbf{\\lambda} = [\\lambda^{(1)}, \\ldots , \\lambda^{(n)}]^{\\operatorname{T}}$, then the decomposition of $\\mathbf{A}$ is:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{A} = \\mathbf{V} \\text{diag(} \\mathbf{\\lambda}\\text{)} \\mathbf{V}^{-1}\n",
    "\\end{align}\n",
    "\n",
    "Constructing a matrix with specific eigenvectors and eigenvalues allows you to stretch space in a specific way. In $\\mathbb{R}^2$, picture a unit circle  (where the set of all unit vectors is $\\mathbf{u}$), showing the existing eigenvectors for a matrix $\\mathbf{A}$. After multiplying $\\mathbf{Au}$, eigenvector $\\mathbf{v}^{(1)}$ will be scaled by $\\lambda_1$ and eigenvector $\\mathbf{v}^{(2)}$ will be scaled by $\\lambda_2$, with the unit circle distorting to encompass those new vector locations.\n",
    "\n",
    "Not all matrices have real-valued eigendecompositions. However, every real symmetric matrix can be decomposed into an expression using only real-valued eigenvectors and eigenvalues:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{A} = \\mathbf{Q} \\mathbf{\\Lambda} \\mathbf{Q}^{\\operatorname{T}}\n",
    "\\end{align}\n",
    "\n",
    "- $\\mathbf{Q}$ is an orthogonal matrix composed of eigenvectors of $\\mathbf{A}$\n",
    "- $\\mathbf{\\Lambda}$ (capital lambda) is a diagonal matrix where the eigenvalue $\\Lambda_{i, i}$ is associated with the eigenvector in column $i$ of $\\mathbf{Q}$, denoted as $\\mathbf{Q}_{:, i}$\n",
    "- Since $\\mathbf{Q}$ is an orthogonal matrix, $\\mathbf{A}$ scales space by $\\lambda_i$ in the direction $\\mathbf{v}^{(i)}$ (akin to the unit circle description above)\n",
    "- While an eigendecomposition will exist for a real symmetric matrix, it isn't guaranteed to be unique\n",
    "\n",
    "What an eigendecomposition of a matrix can tell you:\n",
    "\n",
    "- The matrix is singular if and only if any of the eigenvalues are zero\n",
    "- The decomposition (of real symmetric matrix) can be use to optimize quadratic expressions in the form $f(\\mathbf{x}) = \\mathbf{x}^{\\operatorname{T}} \\mathbf{Ax}$ (where $\\Vert \\mathbf{x} \\Vert_2 = 1$)\n",
    "- A matrix whose eigenvalues are all positive is called **positive definite**. This guarantees that $\\mathbf{x}^{\\operatorname{T}} \\mathbf{Ax} = 0 \\Rightarrow \\mathbf{x} = 0$\n",
    "- A matrix whose eigenvalues are all positive or zero is called **positive semidefinite**, which guarantees that $\\forall \\mathbf{x} \\text{, } \\mathbf{x}^{\\operatorname{T}} \\mathbf{Ax} \\ge 0$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix:\n",
      "[[3. 1.]\n",
      " [0. 2.]]\n",
      "\n",
      "Eigenvalues:\n",
      "[3.+0.j 2.+0.j]\n",
      "\n",
      "Eigenvectors:\n",
      "[[ 1.         -0.70710678]\n",
      " [ 0.          0.70710678]]\n"
     ]
    }
   ],
   "source": [
    "# Eigenvector and eigenvalue example\n",
    "tmp = np.array([3., 1., 0, 2.]).reshape(2, 2)\n",
    "eig_vals = linalg.eig(tmp)\n",
    "\n",
    "print('Matrix:')\n",
    "print(tmp)\n",
    "print()\n",
    "\n",
    "print('Eigenvalues:')\n",
    "print(eig_vals[0])\n",
    "print()\n",
    "\n",
    "print('Eigenvectors:')\n",
    "print(eig_vals[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Singular Value Decomposition\n",
    "\n",
    "- **Singular value decomposition (SVD)** is a way to factor a matrix into singular vectors and singular values. It sheds similar light on a matrix as eigendecomposition, but is more universally applicable - every real matrix has an SVD, even ones that aren't square (a requirement for eigendecomposition)\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{A} = \\mathbf{U}\\mathbf{D}\\mathbf{V}^{\\operatorname{T}}\n",
    "\\end{align}\n",
    "\n",
    "- $\\mathbf{A}$ is shape $m \\times n$\n",
    "- $\\mathbf{U}$ is shape $m \\times m$ and is an orthogonal matrix. Its columns are the **left-singular vectors**\n",
    "- $\\mathbf{D}$ is shape $m \\times n$ (not necessarily square) and is a diagonal matrix. The elements of $\\mathbf{D}$ are the **singular values** of $\\mathbf{A}$\n",
    "- $\\mathbf{V}$ is shape $n \\times n$ and is an orthogonal matrix. Its columns are the **right-singular vectors**\n",
    "- The left-singular vectors of $\\mathbf{A}$ are the eigenvectors of $\\mathbf{A} \\mathbf{A}^{\\operatorname{T}}$\n",
    "- The right-singular vectors of $\\mathbf{A}$ are the eigenvectors of $\\mathbf{A}^{\\operatorname{T}} \\mathbf{A}$\n",
    "- The nonzero singular values of $\\mathbf{A}$ are the square roots of the eigenvalues of both $\\mathbf{A}^{\\mathsf{T}} \\mathbf{A}$ and $\\mathbf{A} \\mathbf{A}^{\\operatorname{T}}$\n",
    "\n",
    "SVD is useful to partially generalize matrix inversion (which isn't defined for non-square matrices)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of matrix A: (3, 2)\n",
      "Matrix A:\n",
      "[[ 5  2]\n",
      " [10  1]\n",
      " [ 0  7]]\n",
      "\n",
      "Shape of U: (3, 3)\n",
      "U:\n",
      "[[-0.46824193  0.0953727  -0.87843813]\n",
      " [-0.86978761 -0.2248469   0.43921906]\n",
      " [-0.15562458  0.96971538  0.18823674]]\n",
      "\n",
      "Shape of s: (2,)\n",
      "s:\n",
      "[11.41254422  6.98239461]\n",
      "\n",
      "Shape of Vh: (2, 2)\n",
      "Vh:\n",
      "[[-0.96727649 -0.25372463]\n",
      " [-0.25372463  0.96727649]]\n"
     ]
    }
   ],
   "source": [
    "# SVD example\n",
    "U, s, Vh = linalg.svd(A)\n",
    "\n",
    "print('Shape of matrix A:', A.shape)\n",
    "print('Matrix A:')\n",
    "print(A)\n",
    "print()\n",
    "\n",
    "print('Shape of U:', U.shape)\n",
    "print('U:')\n",
    "print(U)\n",
    "print()\n",
    "\n",
    "print('Shape of s:', s.shape)\n",
    "print('s:')\n",
    "print(s)\n",
    "print()\n",
    "\n",
    "print('Shape of Vh:', Vh.shape)\n",
    "print('Vh:')\n",
    "print(Vh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvectors of A*A^T (should match cols of U):\n",
      "[[ 0.46824193  0.87843813  0.0953727 ]\n",
      " [ 0.86978761 -0.43921906 -0.2248469 ]\n",
      " [ 0.15562458 -0.18823674  0.96971538]]\n",
      "\n",
      "Left-singular vectors of A (U):\n",
      "[[-0.46824193  0.0953727  -0.87843813]\n",
      " [-0.86978761 -0.2248469   0.43921906]\n",
      " [-0.15562458  0.96971538  0.18823674]]\n",
      "\n",
      "Eigenvectors of A^T*A (should match cols of Vh):\n",
      "[[ 0.96727649 -0.25372463]\n",
      " [ 0.25372463  0.96727649]]\n",
      "\n",
      "Right-singular vectors of A (Vh):\n",
      "[[-0.96727649 -0.25372463]\n",
      " [-0.25372463  0.96727649]]\n",
      "\n",
      "Eigenvalues of A*A^T:\n",
      "[ 1.30246165e+02+0.j -1.86517468e-14+0.j  4.87538345e+01+0.j]\n",
      "Eigenvalues of A^T*A:\n",
      "[130.24616546+0.j  48.75383454+0.j]\n",
      "Squares of singular values of A:\n",
      "[130.24616546  48.75383454]\n"
     ]
    }
   ],
   "source": [
    "# Checking SVD output to eigenvalue claims (true, order is different)\n",
    "A_AT = np.matmul(A, A.T)\n",
    "eig_A_AT = linalg.eig(A_AT)\n",
    "\n",
    "AT_A = np.matmul(A.T, A)\n",
    "eig_AT_A = linalg.eig(AT_A)\n",
    "\n",
    "print('Eigenvectors of A*A^T (should match cols of U):')\n",
    "print(eig_A_AT[1])\n",
    "print()\n",
    "print('Left-singular vectors of A (U):')\n",
    "print(U)\n",
    "print()\n",
    "\n",
    "print('Eigenvectors of A^T*A (should match cols of Vh):')\n",
    "print(eig_AT_A[1])\n",
    "print()\n",
    "print('Right-singular vectors of A (Vh):')\n",
    "print(Vh)\n",
    "print()\n",
    "\n",
    "print('Eigenvalues of A*A^T:')\n",
    "print(eig_A_AT[0])\n",
    "print('Eigenvalues of A^T*A:')\n",
    "print(eig_AT_A[0])\n",
    "print('Squares of singular values of A:')\n",
    "print(s**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Determinant\n",
    "\n",
    "- The **determinant** of a matrix is the factor by which the transformation described by the matrix on the basis vectors stretches or squishes space. In $\\mathbb{R}^2$ you can think of how the area of a given region changes, in $\\mathbb{R}^3$ it's how the volume changes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
